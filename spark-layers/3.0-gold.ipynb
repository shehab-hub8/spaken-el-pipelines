{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4047cdda-1b20-4a37-9f36-3bc1be4aa2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient dim count: 3770\nDepartment dim count: 57\nFact rows: 3777\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, col, expr, current_timestamp, to_timestamp, sha2, concat_ws, coalesce, monotonically_increasing_id\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# ADLS configuration (إعداد المفتاح صحيح هنا)\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.hospitalstorge17.dfs.core.windows.net\",\n",
    "    \"WvLwnc4iFRFgHew6zE9Y55DklpACeOYhA5a3Uz5oCbe16zK0seQxJRs96m5KV4lpDHY56LkqZ3NH+AStfb+C3A==\"\n",
    ")\n",
    "\n",
    "# تصحيح الروابط لتشمل dfs لتتطابق مع المفتاح\n",
    "silver_path = \"abfss://silver@hospitalstorge17.dfs.core.windows.net/silver_real\"\n",
    "gold_dim_patient = \"abfss://gold@hospitalstorge17.dfs.core.windows.net/gold_dim_patient\"\n",
    "gold_dim_department = \"abfss://gold@hospitalstorge17.dfs.core.windows.net/gold_dim_department\"\n",
    "gold_fact = \"abfss://gold@hospitalstorge17.dfs.core.windows.net/gold_fact\"\n",
    "\n",
    "# Read silver data (assume append-only)\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "# Define window for latest admission per patient\n",
    "w = Window.partitionBy(\"patient_id\").orderBy(F.col(\"admission_time\").desc())\n",
    "\n",
    "silver_df = (\n",
    "    silver_df\n",
    "    .withColumn(\"row_num\", F.row_number().over(w))  # Rank by latest admission_time\n",
    "    .filter(F.col(\"row_num\") == 1)                  # Keep only latest row\n",
    "    .drop(\"row_num\")\n",
    ")\n",
    "\n",
    "# Patient Dimension Table Creation\n",
    "# Prepare incoming dimension records (deduplicated per patient, latest record)\n",
    "incoming_patient = (silver_df\n",
    "                    .select(\"patient_id\", \"gender\", \"age\")\n",
    "                    .withColumn(\"effective_from\", current_timestamp())\n",
    "                   )\n",
    "\n",
    "# Create target if not exists\n",
    "if not DeltaTable.isDeltaTable(spark, gold_dim_patient):\n",
    "    # initialize table with schema and empty data\n",
    "    incoming_patient.withColumn(\"surrogate_key\", F.monotonically_increasing_id()) \\\n",
    "                    .withColumn(\"effective_to\", lit(None).cast(\"timestamp\")) \\\n",
    "                    .withColumn(\"is_current\", lit(True)) \\\n",
    "                    .withColumn(\"_hash\", lit(\"init\")) \\\n",
    "                    .write.format(\"delta\").mode(\"overwrite\").save(gold_dim_patient)\n",
    "\n",
    "# Load target as DeltaTable\n",
    "target_patient = DeltaTable.forPath(spark, gold_dim_patient)\n",
    "\n",
    "# Create an expression to detect attribute changes (hash or explicit comparisons)\n",
    "# We'll use a simple concat hash to detect changes\n",
    "incoming_patient = incoming_patient.withColumn(\n",
    "    \"_hash\",\n",
    "    F.sha2(F.concat_ws(\"||\", F.coalesce(col(\"gender\"), lit(\"NA\")), F.coalesce(col(\"age\").cast(\"string\"), lit(\"NA\"))), 256)\n",
    ")\n",
    "\n",
    "# Bring target current hash\n",
    "target_patient_df = spark.read.format(\"delta\").load(gold_dim_patient).withColumn(\n",
    "    \"_target_hash\",\n",
    "    F.sha2(F.concat_ws(\"||\", F.coalesce(col(\"gender\"), lit(\"NA\")), F.coalesce(col(\"age\").cast(\"string\"), lit(\"NA\"))), 256)\n",
    ").select(\"surrogate_key\", \"patient_id\", \"gender\", \"age\", \"is_current\", \"_target_hash\", \"effective_from\", \"effective_to\")\n",
    "\n",
    "# Create temp views for merge\n",
    "incoming_patient.createOrReplaceTempView(\"incoming_patient_tmp\")\n",
    "target_patient_df.createOrReplaceTempView(\"target_patient_tmp\")\n",
    "\n",
    "# We'll implement in two steps using Delta MERGE (safe & explicit)\n",
    "\n",
    "# 1) Mark old current rows as not current where changed\n",
    "changes_df = spark.sql(\"\"\"\n",
    "SELECT t.surrogate_key, t.patient_id\n",
    "FROM target_patient_tmp t\n",
    "JOIN incoming_patient_tmp i\n",
    "  ON t.patient_id = i.patient_id\n",
    "WHERE t.is_current = true AND t._target_hash <> i._hash\n",
    "\"\"\")\n",
    "\n",
    "changed_keys = [row['surrogate_key'] for row in changes_df.collect()]\n",
    "\n",
    "if changed_keys:\n",
    "    # Update existing current records: set is_current=false and effective_to=current_timestamp()\n",
    "    target_patient.update(\n",
    "        condition = expr(\"is_current = true AND surrogate_key IN ({})\".format(\",\".join([str(k) for k in changed_keys]))),\n",
    "        set = {\n",
    "            \"is_current\": expr(\"false\"),\n",
    "            \"effective_to\": expr(\"current_timestamp()\")\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 2) Insert new rows for changed & new records\n",
    "# Build insert DF: join incoming with target to figure new inserts where either not exists or changed\n",
    "inserts_df = spark.sql(\"\"\"\n",
    "SELECT i.patient_id, i.gender, i.age, i.effective_from, i._hash\n",
    "FROM incoming_patient_tmp i\n",
    "LEFT JOIN target_patient_tmp t\n",
    "  ON i.patient_id = t.patient_id AND t.is_current = true\n",
    "WHERE t.patient_id IS NULL OR t._target_hash <> i._hash\n",
    "\"\"\").withColumn(\"surrogate_key\", F.monotonically_increasing_id()) \\\n",
    "  .withColumn(\"effective_to\", lit(None).cast(\"timestamp\")) \\\n",
    "  .withColumn(\"is_current\", lit(True)) \\\n",
    "  .select(\"surrogate_key\", \"patient_id\", \"gender\", \"age\", \"effective_from\", \"effective_to\", \"is_current\")\n",
    "\n",
    "# Append new rows\n",
    "if inserts_df.count() > 0:\n",
    "    inserts_df.write.format(\"delta\").mode(\"append\").save(gold_dim_patient)\n",
    "\n",
    "\n",
    "# Department Dimension Table Creation\n",
    "\n",
    "# prepare incoming (latest per patient feed snapshot)\n",
    "incoming_dept = (silver_df\n",
    "                 .select(\"department\", \"hospital_id\")\n",
    "                )\n",
    "\n",
    "# add hash and dedupe incoming (one row per natural key)\n",
    "incoming_dept = incoming_dept.dropDuplicates([\"department\", \"hospital_id\"]) \\\n",
    "    .withColumn(\"surrogate_key\", monotonically_increasing_id())\n",
    "\n",
    "# initialize table if missing (Simple overwrite for demo, normally merge)\n",
    "incoming_dept.select(\"surrogate_key\", \"department\", \"hospital_id\") \\\n",
    "    .write.format(\"delta\").mode(\"overwrite\").save(gold_dim_department)\n",
    "\n",
    "\n",
    "# Create Fact table\n",
    "\n",
    "# Read current dims (filter is_current=true)\n",
    "dim_patient_df = (spark.read.format(\"delta\").load(gold_dim_patient)\n",
    "                  .filter(col(\"is_current\") == True)\n",
    "                  .select(col(\"surrogate_key\").alias(\"surrogate_key_patient\"), \"patient_id\", \"gender\", \"age\"))\n",
    "\n",
    "dim_dept_df = (spark.read.format(\"delta\").load(gold_dim_department)\n",
    "               .select(col(\"surrogate_key\").alias(\"surrogate_key_dept\"), \"department\", \"hospital_id\"))\n",
    "\n",
    "# Build base fact from silver events\n",
    "fact_base = (silver_df\n",
    "             .select(\"patient_id\", \"department\", \"hospital_id\", \"admission_time\", \"discharge_time\", \"bed_id\")\n",
    "             .withColumn(\"admission_date\", F.to_date(\"admission_time\"))\n",
    "            )\n",
    "\n",
    "# Join to get surrogate keys\n",
    "fact_enriched = (fact_base\n",
    "                 .join(dim_patient_df, on=\"patient_id\", how=\"left\")\n",
    "                 .join(dim_dept_df, on=[\"department\", \"hospital_id\"], how=\"left\")\n",
    "                )\n",
    "\n",
    "# Compute metrics\n",
    "fact_enriched = fact_enriched.withColumn(\"length_of_stay_hours\",\n",
    "                                         (F.unix_timestamp(col(\"discharge_time\")) - F.unix_timestamp(col(\"admission_time\"))) / 3600.0) \\\n",
    "                             .withColumn(\"is_currently_admitted\", F.when(col(\"discharge_time\") > current_timestamp(), lit(True)).otherwise(lit(False))) \\\n",
    "                             .withColumn(\"event_ingestion_time\", current_timestamp())\n",
    "\n",
    "# Let's make column names explicit instead:\n",
    "fact_final = fact_enriched.select(\n",
    "    F.monotonically_increasing_id().alias(\"fact_id\"),\n",
    "    col(\"surrogate_key_patient\").alias(\"patient_sk\"),\n",
    "    col(\"surrogate_key_dept\").alias(\"department_sk\"),\n",
    "    \"admission_time\",\n",
    "    \"discharge_time\",\n",
    "    \"admission_date\",\n",
    "    \"length_of_stay_hours\",\n",
    "    \"is_currently_admitted\",\n",
    "    \"bed_id\",\n",
    "    \"event_ingestion_time\"\n",
    ")\n",
    "\n",
    "# Persist fact table partitioned by admission_date\n",
    "fact_final.write.format(\"delta\").mode(\"overwrite\").save(gold_fact)\n",
    "\n",
    "# Quick sanity checks\n",
    "print(\"Patient dim count:\", spark.read.format(\"delta\").load(gold_dim_patient).count())\n",
    "print(\"Department dim count:\", spark.read.format(\"delta\").load(gold_dim_department).count())\n",
    "print(\"Fact rows:\", spark.read.format(\"delta\").load(gold_fact).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5252515d-66f3-40a4-9499-dcc77e653b6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.0-gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
