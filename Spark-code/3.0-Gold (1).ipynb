{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46a1dd8a-6ba7-4c1d-88b5-c11eb0f8d402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 Starting Gold Layer Processing...\nReading Dimensions from Silver...\nReading Patients Dimension...\nReading Fact Table...\nCalculating Hospital KPIs...\nCalculating Disease Trends...\nCalculating Doctor Workload...\n✅ Saved and Registered: hive_metastore.gold_analytics.hospital_performance_kpi\n✅ Saved and Registered: hive_metastore.gold_analytics.disease_geography_trends\n✅ Saved and Registered: hive_metastore.gold_analytics.doctor_performance_kpi\n\n\uD83C\uDF89 Gold Layer processing finished successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, count, avg, sum, min, max, \n",
    "    datediff, current_timestamp, month, year, desc, when, lit, concat_ws\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. إعدادات المسارات (Configuration)\n",
    "# ---------------------------------------------------------\n",
    "storage_account_name = \"hospitalstorge\"\n",
    "silver_static_path = f\"abfss://silver@{storage_account_name}.dfs.core.windows.net/static_data\"\n",
    "gold_path = f\"abfss://gold@{storage_account_name}.dfs.core.windows.net/kpi_data\"\n",
    "\n",
    "# إعدادات الـ Hive Metastore (الحل المستقر)\n",
    "catalog_name = \"hive_metastore\"\n",
    "db_name = \"gold_analytics\"\n",
    "\n",
    "# إنشاء قاعدة البيانات\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog_name}.{db_name}\")\n",
    "\n",
    "print(\"\uD83D\uDE80 Starting Gold Layer Processing...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. قراءة بيانات Silver (Extraction)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Reading Dimensions from Silver...\")\n",
    "df_hospitals = spark.read.format(\"delta\").load(f\"{silver_static_path}/silver_hospitals\")\n",
    "df_staff = spark.read.format(\"delta\").load(f\"{silver_static_path}/silver_staff\")\n",
    "\n",
    "print(\"Reading Patients Dimension...\")\n",
    "try:\n",
    "    df_patients = spark.read.table(\"silver_data.dim_patients\")\n",
    "except:\n",
    "    print(\"⚠️ Reading dim_patients from Checkpoint path...\")\n",
    "    df_patients = spark.read.format(\"delta\").load(f\"abfss://silver@{storage_account_name}.dfs.core.windows.net/Checkpoints/dim_patients\")\n",
    "\n",
    "print(\"Reading Fact Table...\")\n",
    "try:\n",
    "    df_admissions = spark.read.table(\"silver_data.fact_admissions\")\n",
    "except:\n",
    "    print(\"⚠️ Reading fact_admissions from Checkpoint path...\")\n",
    "    df_admissions = spark.read.format(\"delta\").load(f\"abfss://silver@{storage_account_name}.dfs.core.windows.net/Checkpoints/fact_admissions\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. التحويلات وحساب المقاييس (Transformations & KPIs)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# حساب مدة الإقامة\n",
    "df_fact_enriched = df_admissions.withColumn(\n",
    "    \"length_of_stay_days\", \n",
    "    datediff(col(\"discharge_time\"), col(\"admission_time\"))\n",
    ")\n",
    "\n",
    "# --- KPI 1: ملخص أداء المستشفيات ---\n",
    "print(\"Calculating Hospital KPIs...\")\n",
    "df_gold_hospitals_kpi = df_fact_enriched.join(df_hospitals, \"hospital_id\") \\\n",
    "    .groupBy(\"hospital_name\", \"city\", \"department\") \\\n",
    "    .agg(\n",
    "        count(\"record_id\").alias(\"total_patients\"),\n",
    "        avg(\"length_of_stay_days\").alias(\"avg_stay_days\"),\n",
    "        sum(when(col(\"severity_level\") == \"Critical\", 1).otherwise(0)).alias(\"critical_cases\"),\n",
    "        sum(when(col(\"ICU_admission\") == True, 1).otherwise(0)).alias(\"icu_admissions\")\n",
    "    ) \\\n",
    "    .withColumn(\"last_refreshed\", current_timestamp())\n",
    "\n",
    "# --- KPI 2: تحليل الأمراض (Disease Analysis) - [تم التصحيح هنا] ---\n",
    "print(\"Calculating Disease Trends...\")\n",
    "# نستخدم محافظة المريض (df_patients.governorate)\n",
    "df_gold_disease_trends = df_fact_enriched \\\n",
    "    .join(df_hospitals, \"hospital_id\") \\\n",
    "    .join(df_patients, \"patient_id\") \\\n",
    "    .groupBy(df_patients[\"governorate\"], \"diagnosis\", \"gender\") \\\n",
    "    .agg(\n",
    "        count(\"record_id\").alias(\"case_count\"),\n",
    "        avg(\"length_of_stay_days\").alias(\"avg_treatment_days\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"case_count\"))\n",
    "\n",
    "# --- KPI 3: أداء الأطباء ---\n",
    "print(\"Calculating Doctor Workload...\")\n",
    "df_gold_doctor_stats = df_fact_enriched \\\n",
    "    .join(df_staff, df_fact_enriched.doctor_id == df_staff.staff_id, \"inner\") \\\n",
    "    .join(df_hospitals, df_fact_enriched.hospital_id == df_hospitals.hospital_id, \"inner\") \\\n",
    "    .withColumn(\"doctor_full_name\", concat_ws(\" \", df_staff[\"first_name\"], df_staff[\"last_name\"])) \\\n",
    "    .groupBy(\"doctor_full_name\", \"specialization\", \"hospital_name\") \\\n",
    "    .agg(\n",
    "        count(\"record_id\").alias(\"patients_treated\"),\n",
    "        count(when(col(\"severity_level\") == \"Critical\", 1)).alias(\"critical_cases_handled\")\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. الحفظ في Gold (Loading)\n",
    "# ---------------------------------------------------------\n",
    "def save_to_gold(df, table_name):\n",
    "    full_path = f\"{gold_path}/{table_name}\"\n",
    "    \n",
    "    # 1. حفظ ملفات البيانات\n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .save(full_path)\n",
    "    \n",
    "    # 2. تسجيل الجدول في hive_metastore\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {catalog_name}.{db_name}.{table_name}\n",
    "        USING DELTA LOCATION '{full_path}'\n",
    "    \"\"\")\n",
    "    print(f\"✅ Saved and Registered: {catalog_name}.{db_name}.{table_name}\")\n",
    "\n",
    "# تنفيذ الحفظ\n",
    "save_to_gold(df_gold_hospitals_kpi, \"hospital_performance_kpi\")\n",
    "save_to_gold(df_gold_disease_trends, \"disease_geography_trends\")\n",
    "save_to_gold(df_gold_doctor_stats, \"doctor_performance_kpi\")\n",
    "\n",
    "print(\"\\n\uD83C\uDF89 Gold Layer processing finished successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcd49895-2dc6-4154-af45-1310c0287a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5319939352626567,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3.0-Gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}