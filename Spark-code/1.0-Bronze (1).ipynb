{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc40e237-8000-4a37-9b65-2cd6e3296090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The cluster was unhealthy or has been terminated. Please restart the cluster or attach this notebook to a different cluster.",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, ArrayType\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. إعدادات الاتصال بـ Azure Event Hubs\n",
    "# ---------------------------------------------------------\n",
    "# نستخدم Connection String التي زودتني بها\n",
    "eh_namespace = \"hospital-analytics-namespace5\"\n",
    "eh_topic = \"hospital-analytics-eh\"\n",
    "eh_conn_string = \"Endpoint=sb://hospital-analytics-namespace5.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=li1fiELBAIzuyi73e9U9cQoeicbwrHbch+AEhLDHFfs=\"\n",
    "\n",
    "# إعدادات Kafka الخاصة بـ Spark للاتصال بـ Event Hubs\n",
    "# لاحظ: نستخدم المنفذ 9093 وبروتوكول SASL_SSL\n",
    "# هام: تم استخدام kafkashaded.org.apache.kafka... لحل مشكلة LoginException في Databricks\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": f\"{eh_namespace}.servicebus.windows.net:9093\",\n",
    "    \"subscribe\": eh_topic,\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.jaas.config\": f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{eh_conn_string}\";',\n",
    "    \"startingOffsets\": \"latest\",  # قراءة البيانات الجديدة فقط\n",
    "    \"failOnDataLoss\": \"false\"\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. تعريف هيكل البيانات (Schema) لاستقبال الـ JSON\n",
    "# ---------------------------------------------------------\n",
    "# مطابق لما يرسله كود الـ Producer الخاص بك\n",
    "record_schema = StructType([\n",
    "    StructField(\"record_id\", StringType(), True),\n",
    "    StructField(\"admission_time\", StringType(), True),\n",
    "    StructField(\"discharge_time\", StringType(), True),\n",
    "    StructField(\"hospital_id\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"ICU_admission\", BooleanType(), True),\n",
    "    StructField(\"diagnosis\", StringType(), True),\n",
    "    StructField(\"treatment\", StringType(), True),\n",
    "    StructField(\"arrival_mode\", StringType(), True),\n",
    "    StructField(\"doctor_id\", StringType(), True),\n",
    "    StructField(\"medicine_taken\", StringType(), True),\n",
    "    StructField(\"severity_level\", StringType(), True)\n",
    "])\n",
    "\n",
    "patient_schema = StructType([\n",
    "    StructField(\"patient_id\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"full_name\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"governorate\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"contact_number\", StringType(), True),\n",
    "    StructField(\"records\", ArrayType(record_schema), True)\n",
    "])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. قراءة الدفق (Read Stream)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\uD83D\uDE80 Starting Stream from Event Hubs...\")\n",
    "\n",
    "df_stream = spark.readStream.format(\"kafka\").options(**kafka_options).load()\n",
    "\n",
    "# تحويل البيانات من Binary إلى String ثم إلى JSON Structure\n",
    "df_parsed = df_stream.selectExpr(\"CAST(value AS STRING) as json_string\") \\\n",
    "    .select(from_json(col(\"json_string\"), patient_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"ingestion_time\", current_timestamp())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. الكتابة (Write Stream) إلى Bronze\n",
    "# ---------------------------------------------------------\n",
    "storage_account_name = \"hospitalstorge\"\n",
    "base_path = f\"abfss://bronze@{storage_account_name}.dfs.core.windows.net/Realtime\"\n",
    "checkpoint_path = f\"abfss://bronze@{storage_account_name}.dfs.core.windows.net/Realtime_Checkpoints\"\n",
    "\n",
    "# الكتابة بصيغة Delta (الأفضل لـ Databricks)\n",
    "query = df_parsed.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .option(\"path\", base_path) \\\n",
    "    .start()\n",
    "\n",
    "print(f\"✅ Streaming started! Writing to: {base_path}\")\n",
    "print(f\"\uD83D\uDCC2 Checkpoints at: {checkpoint_path}\")\n",
    "\n",
    "# انتظار الدفق (ضروري لكي لا يتوقف السكريبت)\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2dcfedb-8649-49a1-bfc9-6ca8e227d7b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.0-Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}